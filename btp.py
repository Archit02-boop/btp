# -*- coding: utf-8 -*-
"""BTP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1146k0UiKBHdW1AviEJjPUNWgYmzatkSJ
"""

import numpy as np
from scipy import linalg
from scipy.optimize import minimize
import matplotlib.pyplot as plt

# System parameters
A = np.array([[0, 1, 0, 0],
              [0, -0.1818, 2.6730, 0],
              [0, 0, 0, 1],
              [0, -0.4545, -31.1800, 0]])
B = np.array([[0], [1.818], [0], [4.545]])
C = np.array([[1, 0, 0, 0]])  # We only care about x1

# Simulation parameters
t = np.arange(0, 6, 0.001)
omega = 1
u = np.sin(omega * t)

# Set specific initial conditions for the system
X = np.zeros((4, len(t)))
X[:, 0] = np.random.randn(4)  # Adjusted initial condition

# Simulate the system
def simulate_system(A, B, C, u, t):
    def system(X, t, u):
        return A @ X + B.flatten() * u
    dt = t[1] - t[0]

    for i in range(1, len(t)):
        k1 = system(X[:, i-1], t[i-1], u[i-1])
        k2 = system(X[:, i-1] + 0.5 * dt * k1, t[i-1] + 0.5 * dt, u[i-1])
        k3 = system(X[:, i-1] + 0.5 * dt * k2, t[i-1] + 0.5 * dt, u[i-1])
        k4 = system(X[:, i-1] + dt * k3, t[i], u[i])
        X[:, i] = X[:, i-1] + (dt / 6) * (k1 + 2*k2 + 2*k3 + k4)

    y = C @ X
    return y.flatten()

y = simulate_system(A, B, C, u, t)

# Split data into training and testing sets
train_size = 2000  # Increased training size
y_train, y_test = y[:train_size], y[train_size:]
u_train, u_test = u[:train_size], u[train_size:]
t_train, t_test = t[:train_size], t[train_size:]

# Create Hankel matrix
def create_hankel_matrix(data, q):
    m = len(data) - q + 1
    H = np.zeros((q, m))
    for i in range(q):
        H[i, :] = data[i:i+m]
    return H

# HAVOK parameters
q = 4  # Embedding dimension
r = 4  # Rank of truncated SVD

# Create Hankel matrix and perform truncated SVD
H = create_hankel_matrix(y_train, q)
U, S, Vt = linalg.svd(H, full_matrices=False)
Ur = U[:, :r]
Sr = np.diag(S[:r])
Vr = Vt[:r, :].T

# Compute time derivative of Vr
dVr = (Vr[2:, :] - Vr[:-2, :]) / (2 * 0.001)
Vr_mid = Vr[1:-1, :]

# Create library matrix Theta
u_train_aligned = u_train[q-1:train_size-q+2]  # Align u_train with Vr_mid
Theta = np.column_stack((Vr_mid, u_train_aligned.reshape(-1, 1)))

# Define the objective function for L1 regularized LAD regression
def objective(xi, y, X, lambda_):
    return np.sum(np.abs(y - X @ xi)) + lambda_ * np.sum(np.abs(xi))

# Perform L1 regularized LAD regression
lambda_ = 0.001  # Adjusted regularization parameter
Xi = np.zeros((r, r+1))
for i in range(r):
    result = minimize(objective, np.zeros(r+1), args=(dVr[:, i], Theta, lambda_),
                      method='L-BFGS-B', options={'maxiter': 1000})
    Xi[i, :] = result.x

# Extract identified matrices
A_hat = Xi[:, :r]
B_hat = Xi[:, r].reshape(-1, 1)
C_hat = (Ur @ Sr)[0, :].reshape(1, -1)

# Simulate the identified model
def simulate_identified_model(A_hat, B_hat, C_hat, u, t):
    V = np.zeros((r, len(t)))
    V[:, 0] = np.random.randn(r)
    dt = t[1] - t[0]

    for i in range(1, len(t)):
        dV = A_hat @ V[:, i-1] + B_hat.flatten() * u[i-1]
        V[:, i] = V[:, i-1] + dV * dt

    y_hat = C_hat @ V
    return y_hat.flatten()

y_hat = simulate_identified_model(A_hat, B_hat, C_hat, u, t)

# Plot only the x1 predicted and measured
plt.figure(figsize=(12, 6))
plt.plot(t, y, label='Measured x1 (True System)', alpha=0.7)
plt.plot(t, y_hat, label='Predicted x1 (Identified Model)', alpha=0.7)
plt.axvline(x=t_train[-1], color='r', linestyle='--', label='Train/Test Split')
plt.xlabel('Time')
plt.ylabel('x1')
plt.title('Predicted vs Measured x1 Output')
plt.legend()
plt.grid(True)
plt.show()

# Calculate the normalized error for x1
error = (y - y_hat) / np.max(np.abs(y))

# Plot the normalized error for x1
plt.figure(figsize=(12, 6))
plt.plot(t, error, label='Normalized Error (x1)', alpha=0.7)
plt.xlabel('Time')
plt.ylabel('Normalized Error')
plt.title('Normalized Error between Identified and True x1')
plt.legend()
plt.grid(True)
plt.show()

# Compare eigenvalues
eig_true = np.linalg.eigvals(A)
eig_identified = np.linalg.eigvals(A_hat)

plt.figure(figsize=(8, 8))
plt.scatter(eig_true.real, eig_true.imag, label='True System', marker='o')
plt.scatter(eig_identified.real, eig_identified.imag, label='Identified Model', marker='x')
plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)
plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)
plt.xlabel('Real part')
plt.ylabel('Imaginary part')
plt.title('Eigenvalue Comparison')
plt.legend()
plt.grid(True)
plt.show()

# Compute and print RMSE for training and testing data for x1
rmse_train = np.sqrt(np.mean((y[:train_size] - y_hat[:train_size])**2))
rmse_test = np.sqrt(np.mean((y[train_size:] - y_hat[train_size:])**2))
print(f"RMSE for x1 (Training): {rmse_train}")
print(f"RMSE for x1 (Testing): {rmse_test}")

print("True A matrix:")
print(A)
print("\nIdentified A matrix:")
print(A_hat)
print("\nTrue B matrix:")
print(B)
print("\nIdentified B matrix:")
print(B_hat)
print("\nTrue C matrix:")
print(C)
print("\nIdentified C matrix:")
print(C_hat)